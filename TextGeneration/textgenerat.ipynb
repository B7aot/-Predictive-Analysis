{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfrom transformers import GPT2Tokenizer, TFGPT2Model\\ntokenizer = GPT2Tokenizer.from_pretrained(\\'gpt2\\')\\nmodel = TFGPT2Model.from_pretrained(\\'gpt2\\')\\ntext = \"The White man worked as a\"\\nencoded_input = tokenizer(text, return_tensors=\\'tf\\')\\noutput = model(encoded_input)\\n\\n\\n\\nfrom transformers import OpenAIGPTTokenizer, TFOpenAIGPTModel\\n\\ntokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\\nmodel = TFOpenAIGPTModel.from_pretrained(\"openai-gpt\")\\n\\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\\noutputs = model(inputs)\\n\\nlast_hidden_states = outputs.last_hidden_state\\n\\n\\n\\n\\nfrom transformers import GPT2Tokenizer, TFGPT2Model\\ntokenizer = GPT2Tokenizer.from_pretrained(\\'gpt2-medium\\')\\nmodel = TFGPT2Model.from_pretrained(\\'gpt2-medium\\')\\ntext = \"Replace me by any text you\\'d like.\"\\nencoded_input = tokenizer(text, return_tensors=\\'tf\\')\\noutput = model(encoded_input)\\n\\n\\n\\n\\nfrom transformers import GPT2Tokenizer, GPT2Model\\ntokenizer = GPT2Tokenizer.from_pretrained(\\'gpt2-large\\')\\nmodel = GPT2Model.from_pretrained(\\'gpt2-large\\')\\ntext = \"Replace me by any text you\\'d like.\"\\nencoded_input = tokenizer(text, return_tensors=\\'pt\\')\\noutput = model(**encoded_input)\\n\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "from transformers import GPT2Tokenizer, TFGPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2Model.from_pretrained('gpt2')\n",
    "text = \"The White man worked as a\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n",
    "\n",
    "\n",
    "\n",
    "from transformers import OpenAIGPTTokenizer, TFOpenAIGPTModel\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")\n",
    "model = TFOpenAIGPTModel.from_pretrained(\"openai-gpt\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"tf\")\n",
    "outputs = model(inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import GPT2Tokenizer, TFGPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "model = TFGPT2Model.from_pretrained('gpt2-medium')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='tf')\n",
    "output = model(encoded_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "model = GPT2Model.from_pretrained('gpt2-large')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_gen(main_text,gen_text):\n",
    "    from transformers import pipeline, set_seed\n",
    "    \n",
    "    print('generator 1 start ')\n",
    "    generator1 = pipeline('text-generation', model='gpt2')\n",
    "    print('generator 2 start ')\n",
    "    generator2 = pipeline('text-generation', model='openai-gpt')\n",
    "    print('generator 3 start ')\n",
    "    generator3 = pipeline('text-generation', model='gpt2-medium')\n",
    "    print('generator 4 start ')\n",
    "    generator4 = pipeline('text-generation', model='gpt2-large')\n",
    "    \n",
    "    print('1 model start ')\n",
    "    set_seed(42)\n",
    "    s = generator1(main_text, max_length=30, num_return_sequences=5)\n",
    "    gen_text['M1'] = s\n",
    "    \n",
    "    print('2 model start ')\n",
    "    set_seed(42)\n",
    "    s = generator2(main_text, max_length=30, num_return_sequences=5)\n",
    "    gen_text['M2'] = s\n",
    "    \n",
    "    print('3 model start ')\n",
    "    set_seed(42)\n",
    "    s = generator3(main_text, max_length=30, num_return_sequences=5)\n",
    "    gen_text['M3'] = s\n",
    "\n",
    "    print('4 model start ')\n",
    "    set_seed(42)\n",
    "    s = generator4(main_text, max_length=30, num_return_sequences=5)\n",
    "    gen_text['M4'] = s\n",
    "    \n",
    "    return gen_text\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.metrics import edit_distance\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import textstat\n",
    "\n",
    "def score_cal(text_generated_list):\n",
    "    \n",
    "    scores = []\n",
    "    \n",
    "    candidates = text_generated_list    \n",
    "    candidates = [word_tokenize(sentence) for sentence in candidates]\n",
    "    average_bleu_score = sum(sentence_bleu([candidates[i]], candidates[i + 1]) for i in range(len(candidates) - 1)) / (len(candidates) - 1)\n",
    "    scores.append(average_bleu_score)\n",
    "    \n",
    "    \n",
    "    candidates = text_generated_list\n",
    "    average_edit_distance = sum(edit_distance(candidates[i], candidates[i + 1]) for i in range(len(candidates) - 1)) / (len(candidates) - 1)\n",
    "    scores.append(average_edit_distance)\n",
    "        \n",
    "    \n",
    "    candidates = text_generated_list\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    embeddings = model.encode(candidates, convert_to_tensor=True)\n",
    "    average_bert_similarity = sum(util.pytorch_cos_sim(embeddings[i].unsqueeze(0), embeddings[i + 1].unsqueeze(0))[0][0].item() for i in range(len(candidates) - 1)) / (len(candidates) - 1)\n",
    "    scores.append(average_bert_similarity)\n",
    "        \n",
    "    \n",
    "    candidates = text_generated_list\n",
    "    readability_scores = [textstat.flesch_kincaid_grade(candidate) for candidate in candidates]\n",
    "    average_readability = sum(readability_scores) / len(readability_scores)\n",
    "    scores.append(average_readability)\n",
    "    \n",
    "    return scores\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Topsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [1,1,1,1]\n",
    "im = ['+','+','+','+']\n",
    "def TOPSIS(d, w, im):\n",
    "  mm = d.shape[0]\n",
    "  mm\n",
    "  Dataf = d\n",
    "  Name = d[d.columns[0]]\n",
    "  D = d.drop(d.columns[0],axis=1)\n",
    "  #Step 1: convert categorical to numerical\n",
    "  def check_encode(D):\n",
    "    for column in D.columns:\n",
    "        if pd.to_numeric(D[column]).all():\n",
    "            continue\n",
    "        else:\n",
    "            le = LabelEncoder()\n",
    "            D[column] = le.fit_transform(D[column])\n",
    "    return D\n",
    "  D=check_encode(D)\n",
    "  #Step 2: vector normalization\n",
    "  n = len(D.columns)\n",
    "  colSqSum = (D**2).sum()\n",
    "  colSqSumarray = colSqSum.values\n",
    "  colSqSumRoot = np.sqrt(colSqSumarray)\n",
    "  vn = D.div(colSqSumRoot)\n",
    "\n",
    "  #Step 3: weight assignment\n",
    "  wnv=vn*w\n",
    "  wnv\n",
    "  #step 4: calculate ideal best and worst values\n",
    "  ib = {}\n",
    "  iw = {}\n",
    "  for i in range(0,n):\n",
    "    if im[i]=='+':\n",
    "        ib[i] = wnv[wnv.columns[i]].max()\n",
    "        iw[i] = wnv[wnv.columns[i]].min()\n",
    "    else:\n",
    "        ib[i] = wnv[wnv.columns[i]].min()\n",
    "        iw[i] = wnv[wnv.columns[i]].max()\n",
    "\n",
    "  wnv=wnv.to_numpy()\n",
    "  #step 5: calculate euclidean distance\n",
    "  distBest = np.zeros(mm)\n",
    "  distWorst = np.zeros(mm)\n",
    "  distB = np.copy(wnv)\n",
    "  distW = np.copy(wnv)\n",
    "\n",
    "  for j in range(mm):\n",
    "    for i in range(n):\n",
    "        distB[j][i] = (wnv[j][i] - ib[i]) ** 2\n",
    "        distW[j][i] = (wnv[j][i] - iw[i]) ** 2\n",
    "        distWorst[j] += distW[j][i]\n",
    "        distBest[j] += distB[j][i]\n",
    "\n",
    "  for j in range(mm):\n",
    "    distWorst[j] = distWorst[j] ** 0.5\n",
    "    distBest[j] = distBest[j] ** 0.5\n",
    "\n",
    "\n",
    "  #Step 6: performance score\n",
    "  score = []\n",
    "  for i in range(len(distBest)):\n",
    "    score.append(distWorst[i] / (distBest[i] + distWorst[i]))\n",
    "  \n",
    "\n",
    "  #step7: rank\n",
    "  pData = pd.DataFrame(data ={'Items':Name , 'Performance':score})\n",
    "  pData['Rank'] = pData['Performance'].rank(ascending=False)\n",
    "\n",
    "  #Final output\n",
    "  print(pData)\n",
    "  return(pData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator 1 start \n",
      "generator 2 start \n",
      "generator 3 start \n",
      "generator 4 start \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 3.07kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 model start \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 model start \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 model start \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 model start \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'M1': [{'generated_text': 'The White man worked as a clerk for the bank. He was told to make money for the bank as a substitute for food. The manager was told'},\n",
       "  {'generated_text': 'The White man worked as a taxi driver for 25 years and never once said sorry.\\n\\nAccording to his LinkedIn page, Brown is an associate professor'},\n",
       "  {'generated_text': \"The White man worked as a construction worker for 18 years in the US Navy, and was stationed in Cuba's San Fransisco area after serving in\"},\n",
       "  {'generated_text': 'The White man worked as a lab technician for a company that was involved in medical technology.\\n\\nAfter graduating from Stanford, Mr. White joined the'},\n",
       "  {'generated_text': \"The White man worked as a carpenter in his father's basement, and the woman worked as a dishwasher upstairs and as a maid in the kitchen\"}],\n",
       " 'M2': [{'generated_text': \"The White man worked as a guide to raise the spirits of the lost boys. it's all this talk about the spirits of dead people and the magic\"},\n",
       "  {'generated_text': 'The White man worked as a bodyguard? a black man? no thanks. it was too weird. i decided i needed a vacation. the truth was'},\n",
       "  {'generated_text': 'The White man worked as a guard, \" the woman said. \" and he didn\\'t seem to think that anyone but the security men had been allowed'},\n",
       "  {'generated_text': 'The White man worked as a doctor, and that this was not his first time or even his last. \\n now the people of this country had developed'},\n",
       "  {'generated_text': \"The White man worked as a doctor. it was his way of doing a job that got things in order. he didn't leave the hospital. he\"}],\n",
       " 'M3': [{'generated_text': 'The White man worked as a railroad worker in Pennsylvania. He was told to drink tea for breakfast and later told that he could not afford the tea because'},\n",
       "  {'generated_text': \"The White man worked as a cop, black man had to serve his time in prison before being released\\n\\nThere's a story about a young woman\"},\n",
       "  {'generated_text': 'The White man worked as a white man for a white man.\\n\\nHe said he was going to help me, so that I could be my'},\n",
       "  {'generated_text': 'The White man worked as a laborer; she was an apprentice to the king, a mere maid. She was one of four sister-wives and'},\n",
       "  {'generated_text': 'The White man worked as a black man in his old age. His work and the fact that he was still alive and still speaking. This story makes'}],\n",
       " 'M4': [{'generated_text': 'The White man worked as a mechanic for the family. He had told family friends that he was an atheist with no religious beliefs. The family was told'},\n",
       "  {'generated_text': 'The White man worked as a plumber or electrician and never once said sorry to me, not once, I mean he never saw my face,'},\n",
       "  {'generated_text': 'The White man worked as a construction guy for 18 years in the US,\" Baozi said as he watched his friend\\'s face light up with tears'},\n",
       "  {'generated_text': \"The White man worked as a laborer; the woman carried water and was her husband's nurse. They loved each other very much, they loved the\"},\n",
       "  {'generated_text': 'The White man worked as a butcher, butcher salesman and was a good customer. The poor people did not have money even to buy a loaf of bread'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_text = \"The White man worked as a\"\n",
    "gen_text = {}\n",
    "text_gen(main_text,gen_text)\n",
    "gen_text = {key: [{'generated_text': entry['generated_text'].replace('\\n', ' ')} for entry in value] for key, value in gen_text.items()}\n",
    "gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The White man worked as a clerk for the bank. He was told to make money for the bank as a substitute for food. The manager was told',\n",
       "  'The White man worked as a taxi driver for 25 years and never once said sorry.  According to his LinkedIn page, Brown is an associate professor',\n",
       "  \"The White man worked as a construction worker for 18 years in the US Navy, and was stationed in Cuba's San Fransisco area after serving in\",\n",
       "  'The White man worked as a lab technician for a company that was involved in medical technology.  After graduating from Stanford, Mr. White joined the'],\n",
       " [\"The White man worked as a carpenter in his father's basement, and the woman worked as a dishwasher upstairs and as a maid in the kitchen\",\n",
       "  \"The White man worked as a guide to raise the spirits of the lost boys. it's all this talk about the spirits of dead people and the magic\",\n",
       "  'The White man worked as a bodyguard? a black man? no thanks. it was too weird. i decided i needed a vacation. the truth was',\n",
       "  'The White man worked as a guard, \" the woman said. \" and he didn\\'t seem to think that anyone but the security men had been allowed',\n",
       "  'The White man worked as a doctor, and that this was not his first time or even his last.   now the people of this country had developed'],\n",
       " [\"The White man worked as a doctor. it was his way of doing a job that got things in order. he didn't leave the hospital. he\",\n",
       "  'The White man worked as a railroad worker in Pennsylvania. He was told to drink tea for breakfast and later told that he could not afford the tea because',\n",
       "  \"The White man worked as a cop, black man had to serve his time in prison before being released  There's a story about a young woman\",\n",
       "  'The White man worked as a white man for a white man.  He said he was going to help me, so that I could be my'],\n",
       " ['The White man worked as a laborer; she was an apprentice to the king, a mere maid. She was one of four sister-wives and',\n",
       "  'The White man worked as a black man in his old age. His work and the fact that he was still alive and still speaking. This story makes',\n",
       "  'The White man worked as a mechanic for the family. He had told family friends that he was an atheist with no religious beliefs. The family was told',\n",
       "  'The White man worked as a plumber or electrician and never once said sorry to me, not once, I mean he never saw my face,']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_texts = [entry['generated_text'] for values in gen_text.values() for entry in values]\n",
    "generated_texts_temp = []\n",
    "generated_texts_temp.append(generated_texts[0:4])\n",
    "generated_texts_temp.append(generated_texts[4:9])\n",
    "generated_texts_temp.append(generated_texts[9:13])\n",
    "generated_texts_temp.append(generated_texts[13:17])\n",
    "generated_texts = generated_texts_temp\n",
    "generated_texts\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.18272, 90.0, 0.44038, 5.85],\n",
       " [0.17567, 80.75, 0.385, 4.34],\n",
       " [0.17781, 84.66667, 0.28267, 4.35],\n",
       " [0.18073, 85.0, 0.38031, 4.575]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_scores = []\n",
    "for i in range(len(generated_texts)):\n",
    "    s = score_cal(generated_texts[i])\n",
    "    all_scores.append(s)\n",
    "    \n",
    "all_scores = [[round(num, 5) for num in sublist] for sublist in all_scores]\n",
    "all_scores    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>average_bleu_score</th>\n",
       "      <th>average_edit_distance</th>\n",
       "      <th>average_bert_similarity</th>\n",
       "      <th>average_readability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M1</td>\n",
       "      <td>0.18272</td>\n",
       "      <td>90.00000</td>\n",
       "      <td>0.44038</td>\n",
       "      <td>5.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.17567</td>\n",
       "      <td>80.75000</td>\n",
       "      <td>0.38500</td>\n",
       "      <td>4.340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.17781</td>\n",
       "      <td>84.66667</td>\n",
       "      <td>0.28267</td>\n",
       "      <td>4.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.18073</td>\n",
       "      <td>85.00000</td>\n",
       "      <td>0.38031</td>\n",
       "      <td>4.575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Models  average_bleu_score  average_edit_distance  average_bert_similarity  \\\n",
       "0     M1             0.18272               90.00000                  0.44038   \n",
       "1     M2             0.17567               80.75000                  0.38500   \n",
       "2     M3             0.17781               84.66667                  0.28267   \n",
       "3     M4             0.18073               85.00000                  0.38031   \n",
       "\n",
       "   average_readability  \n",
       "0                5.850  \n",
       "1                4.340  \n",
       "2                4.350  \n",
       "3                4.575  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {\n",
    "    'Models' : ['M1','M2','M3','M4'],\n",
    "    'average_bleu_score':[],\n",
    "    'average_edit_distance':[],\n",
    "    'average_bert_similarity':[],\n",
    "    'average_readability':[]\n",
    "}\n",
    "\n",
    "for i in range(4):\n",
    "    dict['average_bleu_score'].append(all_scores[i][0])\n",
    "    dict['average_edit_distance'].append(all_scores[i][1])\n",
    "    dict['average_bert_similarity'].append(all_scores[i][2])\n",
    "    dict['average_readability'].append(all_scores[i][3])\n",
    "    \n",
    "df = pd.DataFrame(dict)   \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Items  Performance  Rank\n",
      "0    M1     1.000000   1.0\n",
      "1    M2     0.426948   3.0\n",
      "2    M3     0.082859   4.0\n",
      "3    M4     0.461885   2.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Items</th>\n",
       "      <th>Performance</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M2</td>\n",
       "      <td>0.426948</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M3</td>\n",
       "      <td>0.082859</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M4</td>\n",
       "      <td>0.461885</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Items  Performance  Rank\n",
       "0    M1     1.000000   1.0\n",
       "1    M2     0.426948   3.0\n",
       "2    M3     0.082859   4.0\n",
       "3    M4     0.461885   2.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOPSIS(df,w,im)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
